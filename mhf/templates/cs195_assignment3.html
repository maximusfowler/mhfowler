<html>

<head>
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 3: Twitter Sentiment Classification</title>
</head>

<body style="padding:25px;">

<div class="row">
<div class="col-md-2"></div>
<div class="col-md-8">
<div class="page-header center">
    <h1>Twitter Sentiment Classification <small>by <a href="http://github.com/mhfowler">Max Fowler</a></small></h1>
</div>

<h2> My Introduction to Classification and Machine Learning</h2>
<p>
    Before this class and this assignment, I had no idea what machine learning actually was. Turns out
    it's not that bad. In this blog post I'll walk through what I did for this assignment, my conclusions
    about the data I analyzed in this assignment, and some of my
    take-aways about the state of machine learning in general, written for the perspective of someone
    like my pre-assignment self who knows some statistics and has done some programming but doesn't really know what the
    whole data science / machine learning craze is actually about.
    <br><br>

    Long story short, with tools as powerful and well documented as scikit-learn (what I used in this assignment)
    with a background like mine it doesn't take very long to jump in and start drawing real insights from data.
    <br><br>

    What an amazing world we live in!
</p>

<h2>Looking At Tweets</h2>
<p>
    My goal for this assignment was to find out if a machine learning binary classifier could successfully identify
    whether a tweet was happy or sad based on the words that appear in the tweet. The feature space I was using
    to try to classify the tweets was all possible words that appeared in any tweet &mdash; each tweet
    is represented as a vector with length equal to the total number of words that appeared in any of the training
    tweets. In my training set I found 130,159 distinct words appearing in tweets. I also experimented with a few
    different methods of tokenization which collapsed some of these words into each other thusly reducing the
    size of the feature space.
</p>
<p>
    My training data set consisted of 80,000
    tweets which were labeled by hand as happy or sad. After training my models on this data I then tested the efficacy of the models
    by trying to classify an additional 359 test tweets (which were also labeled by hand). My best result
    was a model which was able to correctly classify about 82% of the test tweets... I was impressed.
</p>
<p>
    The original data came from a csv, where each line contained the text of one tweet, a 1 or 0
    polarity label, and some other data (username, when) which I didn't use. I parsed this data into
    python and then used <a href="http://scikit-learn.org/stable/">scikit-learn</a> to train classifiers on this data and
    visualize metrics about the classifiers' performance.
</p>
<img style="width:100%;" src="/static/sci_kit_art.jpg"/>
<p>
    I didn't generate the above images &mdash; the images are from the scikit-learn documentation &mdash; but to me
    they artistically communicate the main thing I learned from this assignment. Each of the four images
    displays how a different model for classification categorizes some data. It doesn't really matter what the
    data is, and this is a very simple classification problem in that it only has 2 dimensions (2 features),
    but the image shows us how there are a lot of different ways you could try to draw lines separating the data
    and there is no clear winner. With the current state of machine learning tools,
    to get the best results you need to experiment with different models, and different ways of tuning
    your models to figure out which model best captures the structures implicit in your data source
    such that you can infer it will perform the best when you move beyond your training data and
    into classifying whatever relevant population you are looking at at large.
</p>

<h2> How To Choose Your Model? </h2>

<p>
    I experimented with three different scikit-learn classifiers: Naive Bayes, Logistic Regression, and Support Vector Machine.
</p>
<p>
    To figure out how well your models are doing you need to test them &mdash; it's pretty much a miniature science experiment
    on your data. Scikit-learn makes it easy to use the classifiers, but it is also clearly important to setup
    a workflow such that you can iteratively get meaningful feedback on how you're classifiers are doing in an
    easy to understand format and in a timely manner.
</p>
<p>
    The metrics I used to evaluate my classifiers were the following:
<ul>
    <li> training mean accuracy: what percentage of the training data did the model classify correctly?</li>
    <li> test mean accuracy: what percentage of the test data did the model classify correctly?</li>
    <li> k-fold cross validation accuracy: by subdividing your training set into k different segments, and then
        using k-1 segments to train your model, leaving the kth segment (k different ways) to test it, you can vet your model for overfitting &mdash; it's trivial to make
        a model which accurately classifies exactly the data it was trained on, getting it to accurately classify the kth
        segment is the real challenge! </li>
    <li> a classification report: presenting the precision (% identified which were correct), recall (% identified
        of all which should have been identified), and f1 score (a weighted average of the two), for both happy and sad tweets (the two categories).</li>
    <li> a confusion matrix: visualizes how many items of each label were mislabeled as which labels.</li>
    <li> an ROC curve: graph of the true positive rate against the false positive rate helps to visualize how efficiently
        the recall of the model increases as the sensitivity increases.</li>
    <li> bottom and top 10 informative tweets: helps give a sanity check as to whether the features which the models are working
        on actually make any sense.</li>
</ul>
</p>

<h2> Visualizing Classifier Performance </h2>
<p>
    Below is a bar graph of the
    10-fold cross-validation accuracy, training accuracy and test accuracy of all three classifiers I experimented with. Purple is Naive Bayes.
    Blue is Logistic Regression. Turquoise is Support Vector Machine. The first three bars are the kcross validation scores,
    the second three bars are the training accuracy scores, the last three bars are the test accuracy scores.
</p>
<br>

<img style="width:100%;" src="/static/shopped_bar_plot.png"/>

<p>
    <br>
    Logistic Regression did the best on kcross validation which I believe is the most important metric. But simple little Naieve Bayes
    managed to take home the win on the very small 369 test samples. Support Vector Machine clearly has an attachment problem,
    grossly overfitting the training data with no regard for all of the possible diversity in the rest of the world.
</p>

<h2> The Full Report - All The Metrics </h2>
<p>
    Below are the full reports of all metrics I calculated for the three different types of classifiers I tested.
</p>
<div class="reports-wrapper" style="width:100%;">
    <div style="width:30%;float:left;">
        <h4>Naive Bayes</h4>
        <p>
            training mean accuracy: 0.8287125<br>
            10 fold cross validation accuracy:<span style="color:green"> 0.761 (+/- 0.006)</span><br>
            test mean accuracy: 0.791086350975<br>
        </p>
        <p>
            CONFUSION MATRIX:<br>
            correct sad recognitions: 139<br>
            correct happy recognitions: 145<br>
            recognized happy as sad: 38<br>
            recognized sad as happy: 37<br>
        </p>
        <p>
            CLASSIFICATION REPORT:<br>
            precision    recall  f1-score   support<br>

            sad tweet:       0.79      0.79      0.79       177<br>
            happy tweet:       0.79      0.80      0.79       182<br>
            avg / total:       0.79      0.79      0.79       359<br>
        </p>
        <p>
            INFORMATIVELY SAD WORDS<br>
            -28.0112	farrah         		<br>
            -20.0000	itchi          		<br>
            -17.9856	fawcett        		<br>
            -17.5747	cancel         		<br>
            -16.5837	sad            		<br>
            -16.1031	throat         	<br>
            -16.0000	postpon        	<br>
            -16.0000	sob            		<br>
            -14.9925	447            <br>
            -14.9925	homesick       <br>
        </p>
        <p>
            INFORMATIVELY HAPPY WORDS
            14.6667	vip<br>
            12.1154	welcom<br>
            12.0000	howdi<br>
            12.0000	zac<br>
            11.8571	congratul<br>
            11.5000	poem<br>
            11.2500	followfridai<br>
            11.0000	fascin<br>
            11.0000	gem<br>
            11.0000	liber<br>
        </p>
    </div>
    <div style="width:30%;margin-left:3%;float:left;">
        <h4>Logistic Regression</h4>
        <p>
            training mean accuracy: 0.8574875<br>
            10 fold cross validation accuracy:<span style="color:green"> 0.776 (+/- 0.009)</span><br>
            test mean accuracy: 0.788300835655<br>
        </p>
        <p>
            CONFUSION MATRIX<br>
            correct sad recognitions: 132<br>
            correct happy recognitions: 151<br>
            recognized happy as sad: 45<br>
            recognized sad as happy: 31<br>
        </p>

        <p>
            CLASSIFICATION REPORT<br>
            precision    recall  f1-score   support<br>
            sad tweet:       0.81      0.75      0.78       177<br>
            happy tweet:       0.77      0.83      0.80       182<br>
            avg / total:       0.79      0.79      0.79       359<br>
        </p>

        <p>
            INFORMATIVELY SAD WORDS<br>
            -3.0612	sad<br>
            -2.9437	cancel<br>
            -2.5743	bummer<br>
            -2.5115	lone<br>
            -2.4582	depress<br>
            -2.2835	disappoint<br>
            -2.2194	fml<br>
            -2.2054	upset<br>
            -2.1793	stink<br>
            -2.1786	miss<br>
        </p>
        <p>
            INFORMATIVELY HAPPY WORDS<br>
            2.1641	proud<br>
            2.1528	yayi<br>
            2.0422	smile<br>
            1.9904	welcom<br>
            1.9431	vision<br>
            1.9298	ull<br>
            1.7921	highlight<br>
            1.7624	dope<br>
            1.7618	heheh<br>
            1.7501	ahaha<br>
        </p>
    </div>
    <div style="width:30%;margin-left:3%;float:left;">
        <h4>Support Vector Machine</h4>

        <p>
            training mean accuracy: 0.9043125<br>
            10 fold cross validation accuracy:<span style="color:green"> 0.757 (+/- 0.009)</span><br>
            test mean accuracy: 0.757660167131<br>
        </p>

        <p>
            CONFUSION MATRIX<br>
            correct sad recognitions: 124<br>
            correct happy recognitions: 148<br>
            recognized happy as sad: 53<br>
            recognized sad as happy: 34<br>
        </p>

        <p>
            CLASSIFICATION REPORT<br>
            precision    recall  f1-score   support<br>
            sad tweet       0.78      0.70      0.74       177<br>
            happy tweet       0.74      0.81      0.77       182<br>
            avg / total       0.76      0.76      0.76       359<br>
        </p>

        <p>
            INFORMATIVELY SAD WORDS<br>
            -2.1682	sauna<br>
            -1.9887	ploblem<br>
            -1.9477	robb<br>
            -1.9406	aof<br>
            -1.9360	fiuhh<br>
            -1.9022	98<br>
            -1.8902	shawneda<br>
            -1.8878	morningwish<br>
            -1.8300	gaultier<br>
            -1.7860	distress<br>
        </p>
        <p>
            INFORMATIVELY HAPPY WORDS<br>
            2.8007	ooff<br>
            2.5854	petunia<br>
            2.3370	bubblewrap<br>
            2.2838	sleepingi<br>
            2.2659	daz<br>
            2.1859	brutu<br>
            2.1575	hartsfield<br>
            2.0950	meryl<br>
            2.0868	4000<br>
            2.0632	contrari<br>
        </p>

    </div>
    <div style="clear:both;"></div>
</div>

<h2> ROC Curves </h2>

<p>
    Below are the ROC curve plots for Naive Bayes and Logistic Regression classifiers.
</p>

<h4> Naive Bayes ROC</h4>
<img style="width:100%;" src="/static/nb_roc.png"/>
<h4> Logistic Regression ROC</h4>
<img style="width:100%;" src="/static/log_roc.png"/>
<p>
    While both graphs have about the same area underneath the curve (LOG=0.8767, NB=0.8841), you can see that they
    have slightly different shapes. Logistic Regression's ROC curve makes more upward progress before it starts
    veering right, visually corresponding with the fact that at LOG's ideal sensitivity (the sensitivity which
    optimizes the ratio of true positives to false positives), LOG performs better than NB does at its ideal sensitivity
    &mdash; LOG performs better than NB.
</p>


<h2> Conclusion </h2>
<p>
    From just looking at the most informative words of the three classifiers (appears in the full reports included above),
    it is immediately clear
    that the informatively sad words of logistic regression sound the saddest, and the informatively happy words
    of logistic regression sound the happiest. I think our intuitive response to the features that a model
    most values is a great proxy towards how well the model will do overall.
</p>
<p>
    While not greatly outperforming the other classifiers logistic regression definitely performed the best,
    its k-cross-validation score being definitely better than the others beyond reasonable doubt of standard deviation.
</p>
<p>
    The SVM classifier clearly overfit the training data &mdash; scoring the highest accuracy on the training data,
    and the lowest accuracy on the test data.
</p>
<p>
    The Naive Bayes classifier actually scored the best accuracy on the test data, but given how much larger the training
    data is compared to the test data, I think the k-cross validation data is a much better estimate of how good
    the classifier is &mdash; my theory is that Naive Bayes just got lucky on the 369 test tweets.
</p>
<p>
    <strong>Among tweets correctly classified by Logistic Regression, below are the 10 tweets with highest predicted probability.</strong>
</p>
<ol>
    <li>(0.999879045518) [sad] @springsingfiend @dvyers @sethdaggett @jlshack AT&amp;T dropped the ball and isn't supporting crap with the new iPhone 3.0... FAIL #att SUCKS!!!</li>
    <li>(0.999571293051) [happy] @sklososky Thanks so much!!! ...from one of your *very* happy Kindle2 winners ; ) I was so surprised, fabulous. Thank you! Best, Kathleen</li>
    <li>(0.999341072006) [sad] I hate Time Warner! Soooo wish I had Vios. Cant watch the fricken Mets game w/o buffering. I feel like im watching free internet porn.</li>
    <li>(0.999065117703) [sad] THE DENTIST LIED! " U WON'T FEEL ANY DISCOMORT! PROB WON'T EVEN NEED PAIN PILLS" MAN U TWIPPIN THIS SHIT HURT!! HOW MANY PILLS CAN I TAKE!!</li>
    <li>(0.9989688352) [sad] NOOOOOOO my DVR just died and I was only half way through the EA presser. Hate you Time Warner</li>
    <li>(0.998837064311) [happy] Obama is quite a good comedian! check out his dinner speech on CNN :) very funny jokes.</li>
    <li>(0.998688003934) [sad] is being fucked by time warner cable. didnt know modems could explode. and Susan Boyle sucks too!</li>
    <li>(0.99841752568) [sad] F*ck Time Warner Cable!!! You f*cking suck balls!!! I have a $700 HD tv &amp; my damn HD channels hardly ever come in. Bullshit!!</li>
    <li>(0.997977519043) [happy] Obama's got JOKES!! haha just got to watch a bit of his after dinner speech from last night... i'm in love with mr. president ;)</li>
    <li>(0.997574098346) [happy] @mashable I never did thank you for including me in your Top 100 Twitter Authors! You Rock! (&amp; I New Wave :-D) http://bit.ly/EOrFV
</ol>

<p>
    <strong>Among tweets incorrectly classified by Logistic Regression, here are the 10 tweets with highest predicted probability.</strong>
</p>
<ol>
    <li>(0.982965768389) [sad] Night at the Museum tonite instead of UP. :( oh well. that 4 yr old better enjoy it. LOL</li>
    <li>(0.968240635576) [happy] My wrist still hurts. I have to get it looked at. I HATE the dr/dentist/scary places. :( Time to watch Eagle eye. If you want to join, txt!</li>
    <li>(0.940737725145) [sad] Back from seeing 'Star Trek' and 'Night at the Museum.' 'Star Trek' was amazing, but 'Night at the Museum' was; eh.</li>
    <li>(0.934841382678) [sad] Life?s a bitch? and so is Dick Cheney. #p2 #bipart #tlot #tcot #hhrs #GOP #DNC http://is.gd/DjyQ</li>
    <li>(0.931188233276) [happy] My dad was in NY for a day, we ate at MESA grill last night and met Bobby Flay. So much fun, except I completely lost my voice today.</li>
    <li>(0.917263423952) [sad] Talk is Cheap: Bing that, I?ll stick with Google. http://bit.ly/XC3C8</li>
    <li>(0.908796573525) [happy] is studing math ;) tomorrow exam and dentist :)</li>
    <li>(0.899535546383) [sad] Give a man a fish, u feed him for the day. Teach him to fish, u feed him for life. Buy him GM, and u F**K him over for good.</li>
    <li>(0.898947580409) [sad] started to think that Citi is in really deep s&amp;^t. Are they gonna survive the turmoil or are they gonna be the next AIG?</li>
    <li>(0.887624889724) [sad] US planning to resume the military tribunals at Guantanamo Bay... only this time those on trial will be AIG execs and Chrysler debt holders</li>

</ol>

<p>
    Reading through both the correctly labeled tweets and the incorrectly labeled tweets it is intuitively very clear why
    they were labelled the way they were. All of the these tweets with high probability have individual words which
    are strongly connotated, many of them have many of these words. As all of the classifiers are use a bag-of-words
    model, this is the best we can hope for from them.
</p>
<p>
    Many of the high probability incorrectly labeled tweets are very sarcastic &mdash; such that they use a lot of words,
    where the tweeter clearly doesn't actually mean what they say. The only way we know the tweeter doesn't mean
    what the words conventionally mean individually is from the meaning of the tweets as a whole, so a bag of words
    model really has no chance at classifying these tweets correctly.
</p>
<p>
    Clearly there are many different layers and scales at which you can optimize a machine learning model, from the statistical
    model you choose to classify a feature space, to feature extraction, to the features you choose to start with.
</p>
<p>
    I wonder how the human brain identifies features as complex and hard to recognize as the tone of a sentence...
</p>


<h2> Extra Credit, Experimenting with Feature Extraction </h2>

<p>
    My main point of experimenting was to try different ways of tokenizing tweets. The idea is that by combining
    different words with equivalent meanings into the same feature, the algorithms ought to be able to better recognize patterns. For instance,
    if excited and excitedddd appear in two different tweets, they probably both mean kind of the same thing, and
    so anything the algorithm has 'learned' about a tweet containing the word excited should also apply to a tweet
    containing the word excitedddd.
</p>

<p>
    It took me a while to figure out what metric was the best for evaluating the effectiveness of the tokenizer. I
    was thrown off at first because the test mean accuracy was actually lower for most of my feature extraction
    techniques. I eventually realized that the test data set was much smaller than the training data set, and that
    the 10 fold cross validation accuracy measured on the training set was actually a much better metric of how
    my classifiers were performing &mdash; and indeed with improved (rational) feature extraction the 10 fold
    cross validation accuracy improved!
</p>
<p>
    From the reports below you will also notice that every time the test mean accuracy score is higher than the k-validation score.
    If there was an even distribution of types of tweets between the training and the test data theoretically these numbers
    should be about the same. However, for this particular data, there is a higher percentage of happy tweets in the test data
    than in the training data and all of the models are better at identifying happy tweets than sad tweets (they have
    higher f1-scores for happy tweets than sad tweets) &mdash; this results in a higher average accuracy for the test data
    than for the k-cross validation score on the training data (the training data accuracy is still higher due to overfitting).
</p>

<p>
    This experiment was great practice in classifier analysis and gave me a good sense
    for how to make use of classifier metrics &mdash; my main take away from this assignment is that figuring out how
    to use classifier metrics to help you select the best model (or tuning or feature extraction) to use is the main skill in the 'art'
    of machine learning.
</p>

<p> Below are the training mean scores, test mean scores, and kcross validation scores for different methods of
    tokenization I implemented with the three different classifiers.</p>

<br>
<h4> just lowercasing words (baseline)</h4>
**************** nb ****************<br>
training mean accuracy: 0.903125<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.821727019499<br>
**************** log ****************<br>
training mean accuracy: 0.92675<br>
<span style="color:blue;">10 fold cross validation accuracy: 0.77 (+/- 0.01) <------- baseline </span><br>
test mean accuracy: 0.791086350975<br>
**************** svm ****************<br>
training mean accuracy: 0.9861375<br>
10 fold cross validation accuracy: 0.75 (+/- 0.01)<br>
test mean accuracy: 0.771587743733<br>
<br>
<h4> lowercasing words and using a stemmer </h4>
**************** nb ****************<br>
training mean accuracy: 0.8963375<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.802228412256<br>
**************** log ****************<br>
training mean accuracy: 0.9189125<br>
10 fold cross validation accuracy: 0.77 (+/- 0.01)<br>
test mean accuracy: 0.791086350975<br>
**************** svm ****************<br>
training mean accuracy: 0.982725<br>
10 fold cross validation accuracy: 0.75 (+/- 0.00)<br>
test mean accuracy: 0.768802228412<br>
<br>
<h4> lowercasing words and removing all punctuation </h4>
**************** nb ****************<br>
training mean accuracy: 0.87825<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.796657381616<br>
**************** log ****************<br>
training mean accuracy: 0.9008375<br>
10 fold cross validation accuracy: 0.77 (+/- 0.01)<br>
test mean accuracy: 0.807799442897<br>
**************** svm ****************<br>
training mean accuracy: 0.969475<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.754874651811<br>
<br>
<h4> lowercasing words and replacing all urls (containg www. or http) with "url" and all @user with "atuser" </h4>
**************** nb ****************<br>
training mean accuracy: 0.8692875<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.813370473538<br>
**************** log ****************<br>
training mean accuracy: 0.89985<br>
10 fold cross validation accuracy: 0.77 (+/- 0.01)<br>
test mean accuracy: 0.802228412256<br>
**************** svm ****************<br>
training mean accuracy: 0.9604625<br>
10 fold cross validation accuracy: 0.75 (+/- 0.01)<br>
test mean accuracy: 0.774373259053<br>
<br>
<h4> all feature selection techniques combined </h4>
**************** nb ****************<br>
training mean accuracy: 0.8287125<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.791086350975<br>
**************** log ****************<br>
training mean accuracy: 0.8574875<br>
<span style="color:green;">10 fold cross validation accuracy: 0.78 (+/- 0.01) <----- best result</span><br>
test mean accuracy: 0.788300835655<br>
**************** svm ****************<br>
training mean accuracy: 0.9043125<br>
10 fold cross validation accuracy: 0.76 (+/- 0.01)<br>
test mean accuracy: 0.757660167131<br>
<br>


<div class="col-md-2"></div>

</div>

</body>
</html>